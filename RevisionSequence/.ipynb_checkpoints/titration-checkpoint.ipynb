{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import scipy as sp\n",
    "from scipy import stats, optimize, interpolate\n",
    "import statsmodels.api as sm\n",
    "import statsmodels as sm\n",
    "random.seed(0)\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leahbriscoe/Documents/MicroBatch/microbatch_vc/data/Gibbonsr_complete_otu/titration/ folder already exists.\n"
     ]
    }
   ],
   "source": [
    "main_dir =\"/u/home/b/briscoel/project-halperin/MicroBatch/data/\"\n",
    "script_folder= \"/u/home/b/briscoel/project-halperin/MicroBatch/RevisionSequence/\"\n",
    "local = True\n",
    "\n",
    "sys.argv = [ \"Gibbonsr_complete_otu\",\"rel\",\"\"]\n",
    "\n",
    "if local:\n",
    "    main_dir = \"/Users/leahbriscoe/Documents/MicroBatch/microbatch_vc/data/\"\n",
    "    script_folder = \"/Users/leahbriscoe/Documents/MicroBatch/microbatch_vc/RevisionSequence/\"\n",
    "\n",
    "folder = sys.argv[0] # \"AGPr_max_k5\" #\"AGPr_complete_otu\" \n",
    "trans = sys.argv[1] #\"rel\"\n",
    "\n",
    "data_dir = main_dir + folder + \"/\"\n",
    "\n",
    "\n",
    "out_dir = data_dir + \"titration/\"\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(\"created folder : \", out_dir)\n",
    "\n",
    "else:\n",
    "    print(out_dir, \"folder already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_csv(data_dir + \"metadata.txt\", delimiter = \"\\t\",header=0)\n",
    "\n",
    "all_studies = list(Counter(metadata_table['study']))\n",
    "\n",
    "if folder == \"Gibbonsr_complete_otu\":\n",
    "    phenotype ='bin_crc_normal'\n",
    "    batch=\"study\"\n",
    "    metadata_table[phenotype] = [1 if lab=='CRC' else 0 for lab in metadata_table['bin_crc_normal']]\n",
    "elif folder == \"Thomasr_complete_otu\":\n",
    "    phenotype ='bin_crc_normal'\n",
    "    batch=\"dataset_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_control_samples = list(metadata_table[metadata_table['bin_crc_normal'] == 0].index)\n",
    "#all_case_samples = list(metadata_table[metadata_table['bin_crc_normal'] == 1].index)\n",
    "#feature_table_presence = feature_table  > 0  \n",
    "# control_presence_proportion = (feature_table_presence[all_control_samples].sum(axis =1))/len(all_control_samples)\n",
    "# case_presence_proportion = (feature_table_presence[all_case_samples].sum(axis =1))/len(all_case_samples)\n",
    "# ## FILTER FOR 1/3 in at least case OR control : not using for now\n",
    "# prevalence_filter = np.array(control_presence_proportion > 0.33) | np.array(case_presence_proportion > 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orig_feature_table = pd.read_csv(data_dir  +\"feature_table_\" + trans + \".txt\" ,delimiter = \"\\t\",header=0)\n",
    "df_biomarkers = pd.DataFrame(index = orig_feature_table.index,columns = all_studies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titration(d1, d2 ,metadata_table,fractions,corrections):\n",
    "    random.seed(0)\n",
    "    control_metadata1 = metadata_table[(metadata_table[phenotype] == 0 ) & (metadata_table[batch] ==  d1) ]\n",
    "    case_metadata1 = metadata_table[(metadata_table[phenotype] == 1) & (metadata_table[batch] ==  d1)]\n",
    "\n",
    "    control_metadata2 = metadata_table[(metadata_table[phenotype] == 0 ) & (metadata_table[batch] ==  d2) ]\n",
    "    case_metadata2 = metadata_table[(metadata_table[phenotype] == 1) & (metadata_table[batch] ==  d2)]\n",
    "\n",
    "    control_samples_d1 = list(control_metadata1.index)\n",
    "    case_samples_d1 = list(case_metadata1.index)\n",
    "\n",
    "    control_samples_d2 = list(control_metadata2.index)\n",
    "    case_samples_d2 = list(case_metadata2.index)\n",
    "\n",
    "    min_sample_size = min([len(control_samples_d1),len(case_samples_d1),len(control_samples_d2),len(case_samples_d2)])\n",
    "    \n",
    "    control_samples_d1 = random.sample(control_samples_d1,min_sample_size)\n",
    "    case_samples_d1 = random.sample(case_samples_d1,min_sample_size)\n",
    "    control_samples_d2 = random.sample(control_samples_d2,min_sample_size)\n",
    "    case_samples_d2 = random.sample(case_samples_d2,min_sample_size)\n",
    "    \n",
    "    \n",
    "    replicate_titrate_rel_names = {}\n",
    "    replicate_titrate_rel_fdr = {}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for correction in corrections:\n",
    "        print(correction)\n",
    "        replicate_titrate_rel_names[correction] = dict()\n",
    "        replicate_titrate_rel_fdr[correction] = dict()\n",
    "        if correction == \"nocorrection\":\n",
    "            feature_table = pd.read_csv(data_dir  + \"feature_table_\" + trans + \".txt\" ,delimiter = \"\\t\",header=0)\n",
    "        else:\n",
    "            feature_table = pd.read_csv(data_dir  +\"feature_table_\" + trans + \"_\" + correction + \".txt\" ,delimiter = \"\\t\",header=0)\n",
    "\n",
    "        for n in range(num_replicates):\n",
    "            print(n)\n",
    "            titrate_rel_names = {}\n",
    "            titrate_rel_fdr = {}\n",
    "            for f in fractions:\n",
    "                pvals = []           \n",
    "                case_samples = case_samples_d1\n",
    "                control_samples = random.sample(control_samples_d1,int((1-f) * min_sample_size)) + random.sample(control_samples_d2, int(f * min_sample_size))\n",
    "                feature_table_titrate_control = feature_table[control_samples ]\n",
    "                feature_table_titrate_case = feature_table[case_samples ]\n",
    "                \n",
    "                feature_table_titrate_control_prev = (feature_table_titrate_control > 1e-4)\n",
    "                feature_table_titrate_case_prev = (feature_table_titrate_case > 1e-4)\n",
    "\n",
    "                # for each taxa\n",
    "                tax_names = []\n",
    "                for tax in range(feature_table_titrate_case.shape[0]):\n",
    "                    \n",
    "                    if feature_table_titrate_control_prev.iloc[tax,:].astype(bool).sum()/float(feature_table_titrate_control.shape[1]) >= 1/3.0 or feature_table_titrate_case_prev.iloc[tax,:].astype(bool).sum()/float(feature_table_titrate_case.shape[1]) >= 1/3.0:\n",
    "                        pvals.append(sp.stats.ranksums(feature_table_titrate_control.iloc[tax,:],feature_table_titrate_case.iloc[tax,:])[1])\n",
    "                        tax_names.append(feature_table_titrate_case.index[tax])\n",
    "                    #else:\n",
    "                        #pvals.append(1)\n",
    "#                     if (feature_table_titrate_control.iloc[tax,:] > -2).astype(bool).sum()/float(feature_table_titrate_control.shape[1]) >= 1/3.0 or (feature_table_titrate_case.iloc[tax,:] > -2).astype(bool).sum()/float(feature_table_titrate_case.shape[1]) >= 1/3.0:\n",
    "#                         pvals.append(sp.stats.ranksums(feature_table_titrate_control.iloc[tax,:],feature_table_titrate_case.iloc[tax,:])[1])\n",
    "#                     else:\n",
    "#                         pvals.append(1)\n",
    "\n",
    "                pvals = np.array(pvals)\n",
    "                if(len(pvals) > 0):\n",
    "    \n",
    "                    pvals_fdr = sm.sandbox.stats.multicomp.multipletests(np.array(pvals),alpha=0.05,method='fdr_bh')[1]\n",
    "                else:\n",
    "                    pvals_fdr = []\n",
    "                names = []\n",
    "                fdr = []\n",
    "\n",
    "                for j in range(len(pvals_fdr)):\n",
    "                    if pvals_fdr[j] <= 0.05:\n",
    "                        names.append(tax_names[j])\n",
    "                        fdr.append(pvals_fdr[j])\n",
    "\n",
    "\n",
    "                titrate_rel_names[f] = names\n",
    "                titrate_rel_fdr[f] = fdr\n",
    "\n",
    "\n",
    "\n",
    "            replicate_titrate_rel_names[correction][n] = titrate_rel_names\n",
    "            replicate_titrate_rel_fdr[correction][n] = titrate_rel_fdr\n",
    "            \n",
    "    plot_titrate_rel_names = dict()\n",
    "    for correction in corrections:\n",
    "        plot_titrate_rel_names[correction] = dict()\n",
    "        plot_titrate_rel_names[correction][\"fraction\"]  = []\n",
    "        plot_titrate_rel_names[correction][\"num_sig\"]  = []\n",
    "        for f in fractions:\n",
    "            plot_titrate_rel_names[correction][\"fraction\"].append(f)\n",
    "            num_sig = []\n",
    "            for n in range(num_replicates):\n",
    "                num_sig.append(len(replicate_titrate_rel_fdr[correction][n][f]))\n",
    "\n",
    "            # calculate mean number of significant OTUs\n",
    "            plot_titrate_rel_names[correction][\"num_sig\"].append(np.mean(num_sig))\n",
    "    return(plot_titrate_rel_names,replicate_titrate_rel_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# fractions = [0,.25 ,  1] # fraction of controls replaced\n",
    "# corrections = [\"nocorrection\",\"percentilenorm\"] \n",
    "# titrate1,biomarkers1 = titration(d1 = d11, d2 = d21, \n",
    "#                                    metadata_table = metadata_table,fractions = fractions,corrections = corrections)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nocorrection\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "num_replicates= 5\n",
    "\n",
    "fractions = [0,.25 , .50, 0.75, 1] # fraction of controls replaced\n",
    "corrections = [\"nocorrection\",\"percentilenorm\",\"limmacounts\",\"combatcounts\",\"clr_pcacounts\",\"DCC\"] \n",
    "corrections_copy = corrections\n",
    "\n",
    "\n",
    "for s in range(len(all_studies)):\n",
    "    for s2 in range((s+1),len(all_studies)):\n",
    "        d11 = all_studies[s] # 'HanniganGD_2017'\n",
    "        d21 = all_studies[s2] #'YuJ_2015'\n",
    "        d12 = all_studies[s2] #'YuJ_2015'\n",
    "        d22 = all_studies[s] #'HanniganGD_2017'\n",
    "\n",
    "        \n",
    "        titrate1,biomarkers1 = titration(d1 = d11, d2 = d21, \n",
    "                                   metadata_table = metadata_table,fractions = fractions,corrections = corrections)\n",
    "        \n",
    "        biomarker_list_list = []\n",
    "        for i in range(num_replicates):\n",
    "            biomarker_list_list.append(biomarkers1[\"clr_pcacounts\"][i][0])\n",
    "\n",
    "        agreement_onedataset = list(reduce(set.intersection, [set(item) for item in biomarker_list_list ]))\n",
    "        df_biomarkers.loc[agreement_onedataset,d11] = 1\n",
    "        \n",
    "        df_biomarkers.to_csv( \"DF_biomarkers.csv\",index=True)\n",
    "\n",
    "        titrate2,biomarkers2 = titration(d1 = d12 , d2 = d22, \n",
    "                                   metadata_table = metadata_table,fractions = fractions,corrections = corrections)\n",
    "        \n",
    "                #corrections = [\"nocorrection\",\"percentilenorm\",\"clr_pcacounts\"] \n",
    "        corrections = corrections_copy\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2,sharex=True, sharey=True)  \n",
    "\n",
    "        line_widths = [3.5,3.5,3.5,3.5,3.5,4.5]\n",
    "        line_styles = ['-','-',\":\",\"--\",\"--\",'-.']\n",
    "\n",
    "        for c in range(len(corrections)):\n",
    "            ax1.plot(titrate1[corrections[c]][\"fraction\"],titrate1[corrections[c]][\"num_sig\"],\n",
    "                     label = corrections[c],linewidth=line_widths[c],linestyle = line_styles[c])\n",
    "        d1 = d11\n",
    "        d2 = d21\n",
    "        ax1.set_title(d1 + ' > titrate ' + d2 ,fontsize=9)\n",
    "\n",
    "        for c in range(len(corrections)):\n",
    "            ax2.plot(titrate2[corrections[c]][\"fraction\"],titrate2[corrections[c]][\"num_sig\"],\n",
    "                     label = corrections[c],linewidth=line_widths[c],linestyle = line_styles[c])\n",
    "\n",
    "        d2 = d22\n",
    "        d1 = d12 \n",
    "        ax2.set_title(d1 + ' > titrate ' + d2 ,fontsize=9)\n",
    "\n",
    "        ax1.locator_params(nbins=5, axis='y')\n",
    "        ax2.legend(loc=2,prop={'size':10})\n",
    "        #fig.text(0.001, 0.5, 'number of significant OTUs (q <= 0.05)', va='center', rotation='vertical',fontsize=12)\n",
    "        fig.text(0.5, 0.003, 'fraction of control samples from another study', ha='center',fontsize=12)\n",
    "        ax1.set_ylabel('number of significant OTUs (q <= 0.05)')\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        \n",
    "        plt.savefig(out_dir  + 'titrate_' + d11 + '_' + d21 + '.pdf',dpi=300,format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_studies)\n",
    "for s in range(len(all_studies)):\n",
    "    for s2 in range((s+1),len(all_studies)):\n",
    "        print(all_studies[s] + \" \" + all_studies[s2])\n",
    "        print(all_studies[s2] + \" \" + all_studies[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correction = \"clr_scale_pca\"\n",
    "#feature_table = pd.read_csv(data_dir  +\"feature_table_\" + trans + \"_\" + correction + \".txt\" ,delimiter = \"\\t\",header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(feature_table > 1e-4).iloc[50,:].astype(bool).sum()/feature_table.shape[1]\n",
    "\n",
    "#feature_table_titrate_control.iloc[tax,:].astype(bool).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(feature_table.iloc[50,:].astype(bool).sum()/feature_table.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
